{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195 (512, 512)\n"
     ]
    }
   ],
   "source": [
    "import dicom\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "\n",
    "data_dir='D:/stage1/'\n",
    "patients=os.listdir(data_dir)\n",
    "labels_df=pd.read_csv('D:/stage1_labels.csv',index_col=0)\n",
    "MODEL_NAME = 'lungcancerdetect-{}-{}.model'.format(1e-7, '2conv-basic')\n",
    "much_data = np.load('D:/muchdata-50-50-20.npy')\n",
    "for patient in patients[:1]:\n",
    "    label=labels_df.get_value(patient,'cancer')\n",
    "    path=data_dir+patient\n",
    "    slices=[dicom.read_file(path+'/'+s)for s in os.listdir(path)]\n",
    "    slices.sort(key=lambda x:int(x.ImagePositionPatient[2]))\n",
    "    print(len(slices),slices[0].pixel_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-7aebd6748fc6>:82: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Epoch 1 completed out of 10 loss: 0.0\n",
      "Accuracy: 0.65\n",
      "Epoch 2 completed out of 10 loss: 0.0\n",
      "Accuracy: 0.65\n",
      "Epoch 3 completed out of 10 loss: 0.0\n",
      "Accuracy: 0.7\n",
      "Epoch 4 completed out of 10 loss: 0.0\n",
      "Accuracy: 0.65\n",
      "Epoch 5 completed out of 10 loss: 0.0\n",
      "Accuracy: 0.75\n",
      "Epoch 6 completed out of 10 loss: 469666336.0\n",
      "Accuracy: 0.8\n",
      "Epoch 7 completed out of 10 loss: 239130592.0\n",
      "Accuracy: 0.85\n",
      "Epoch 8 completed out of 10 loss: 0.0\n",
      "Accuracy: 0.75\n",
      "Epoch 9 completed out of 10 loss: 0.0\n",
      "Accuracy: 0.85\n",
      "Epoch 10 completed out of 10 loss: 0.0\n",
      "Accuracy: 0.9\n",
      "Done. Finishing accuracy:\n",
      "Accuracy: 0.85\n",
      "fitment percent: 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "IMG_SIZE_PX=50\n",
    "SLICE_COUNT=20\n",
    "\n",
    "n_classes=2\n",
    "\n",
    "x=tf.placeholder('float')\n",
    "y=tf.placeholder('float')\n",
    "\n",
    "keep_rate=0.8\n",
    "keep_prob=tf.placeholder(tf.float32)\n",
    "\n",
    "def conv3d(x, W):\n",
    "    return tf.nn.conv3d(x, W, strides=[1,1,1,1,1], padding='SAME')\n",
    "\n",
    "def maxpool3d(x):\n",
    "    \n",
    "    return tf.nn.max_pool3d(x, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='SAME')\n",
    "\n",
    "def convolutional_neural_network(x):\n",
    "    \n",
    "    weights = {'W_conv1':tf.Variable(tf.random_normal([3,3,3,1,32])),\n",
    "               \n",
    "               'W_conv2':tf.Variable(tf.random_normal([3,3,3,32,64])),\n",
    "               #'W_conv3':tf.Variable(tf.random_normal([3,3,3,64,128])),\n",
    "               #'W_conv4':tf.Variable(tf.random_normal([5,5,5,128,256])),\n",
    "               #'W_conv5':tf.Variable(tf.random_normal([5,5,5,256,512])),\n",
    "               #'W_conv6':tf.Variable(tf.random_normal([5,5,5,512,1024])),\n",
    "              # 'W_conv7':tf.Variable(tf.random_normal([5,5,5,1024,2048])),\n",
    "               \n",
    "               'W_fc':tf.Variable(tf.random_normal([ 54080 ,1024])),\n",
    "               'out':tf.Variable(tf.random_normal([1024, n_classes]))}\n",
    "\n",
    "    biases = {'b_conv1':tf.Variable(tf.random_normal([32])),\n",
    "               'b_conv2':tf.Variable(tf.random_normal([64])),\n",
    "               #'b_conv3':tf.Variable(tf.random_normal([128])),\n",
    "               #'b_conv4':tf.Variable(tf.random_normal([256])),\n",
    "               #'b_conv5':tf.Variable(tf.random_normal([512])),\n",
    "               #'b_conv6':tf.Variable(tf.random_normal([1024])),\n",
    "               #'b_conv7':tf.Variable(tf.random_normal([2048])),\n",
    "               'b_fc':tf.Variable(tf.random_normal([1024])),\n",
    "               'out':tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "    \n",
    "    x = tf.reshape(x, shape=[-1, IMG_SIZE_PX, IMG_SIZE_PX, SLICE_COUNT, 1])\n",
    "\n",
    "    conv1 = tf.nn.relu(conv3d(x, weights['W_conv1']) + biases['b_conv1'])\n",
    "    conv1 = maxpool3d(conv1)\n",
    "    conv2 = tf.nn.relu(conv3d(conv1, weights['W_conv2']) + biases['b_conv2'])\n",
    "    conv2 = maxpool3d(conv2)\n",
    "    #conv3 = tf.nn.relu(conv3d(conv2, weights['W_conv3']) + biases['b_conv3'])\n",
    "    #conv3 = maxpool3d(conv3)\n",
    "    #conv4 = tf.nn.relu(conv3d(conv3, weights['W_conv4']) + biases['b_conv4'])\n",
    "    #conv4 = maxpool3d(conv4)\n",
    "    #conv5 = tf.nn.relu(conv3d(conv4, weights['W_conv5']) + biases['b_conv5'])\n",
    "    #conv5 = maxpool3d(conv5)\n",
    "    #conv6 = tf.nn.relu(conv3d(conv5, weights['W_conv6']) + biases['b_conv6'])\n",
    "    #conv6 = maxpool3d(conv6)\n",
    "    #conv7 = tf.nn.relu(conv3d(conv6, weights['W_conv7']) + biases['b_conv7'])\n",
    "    #conv7 = maxpool3d(conv7)\n",
    "    \n",
    "    \n",
    "\n",
    "    fc = tf.reshape(conv2,[-1,  54080 ])\n",
    "    fc = tf.nn.relu(tf.matmul(fc, weights['W_fc'])+biases['b_fc'])\n",
    "    fc = tf.nn.dropout(fc, keep_rate)\n",
    "\n",
    "    output = tf.matmul(fc, weights['out'])+biases['out']\n",
    "    \n",
    "\n",
    "    return output\n",
    "much_data = np.load('D:/muchdata-50-50-20.npy')\n",
    "train_data = much_data[-100:]\n",
    "validation_data = much_data[-20:]\n",
    "def train_neural_network(x):\n",
    "    prediction = convolutional_neural_network(x)\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        successful_runs = 0\n",
    "        total_runs = 0\n",
    "        hm_epochs = 10\n",
    "        \n",
    "        for epoch in range(hm_epochs):\n",
    "            for data in train_data:\n",
    "                total_runs += 1\n",
    "                epoch_loss = 0\n",
    "                X = data[0]\n",
    "                Y = data[1]\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: X, y: Y})\n",
    "                epoch_loss += c\n",
    "                successful_runs += 1\n",
    "            print('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "            correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "            print('Accuracy:',accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]}))\n",
    "        print('Done. Finishing accuracy:')\n",
    "        print('Accuracy:',accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]}))\n",
    "        \n",
    "        print('fitment percent:',successful_runs/total_runs)    \n",
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "IMG_SIZE_PX = 50\n",
    "SLICE_COUNT = 20\n",
    "\n",
    "n_classes = 2\n",
    "batch_size = 10\n",
    "\n",
    "x = tf.placeholder('float')\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "keep_rate = 0.8\n",
    "\n",
    "def conv3d(x, W):\n",
    "    return tf.nn.conv3d(x, W, strides=[1,1,1,1,1], padding='SAME')\n",
    "\n",
    "def maxpool3d(x):\n",
    "    #                        size of window         movement of window as you slide about\n",
    "    return tf.nn.max_pool3d(x, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convolutional_neural_network(x):\n",
    "    #                # 5 x 5 x 5 patches, 1 channel, 32 features to compute.\n",
    "    weights = {'W_conv1':tf.Variable(tf.random_normal([3,3,3,1,32])),\n",
    "               #       5 x 5 x 5 patches, 32 channels, 64 features to compute.\n",
    "               'W_conv2':tf.Variable(tf.random_normal([3,3,3,32,64])),\n",
    "               #'W_conv3':tf.Variable(tf.random_normal([3,3,3,64,128])),\n",
    "               #'W_conv4':tf.Variable(tf.random_normal([3,3,3,128,256])),\n",
    "               #                                  64 features\n",
    "               'W_fc':tf.Variable(tf.random_normal([54080,1024])),\n",
    "               'out':tf.Variable(tf.random_normal([1024, n_classes]))}\n",
    "\n",
    "    biases = {'b_conv1':tf.Variable(tf.random_normal([32])),\n",
    "               'b_conv2':tf.Variable(tf.random_normal([64])),\n",
    "             # 'b_conv3':tf.Variable(tf.random_normal([128])),\n",
    "             # 'b_conv4':tf.Variable(tf.random_normal([256])),\n",
    "               'b_fc':tf.Variable(tf.random_normal([1024])),\n",
    "               'out':tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "    #                            image X      image Y        image Z\n",
    "    x = tf.reshape(x, shape=[-1, IMG_SIZE_PX, IMG_SIZE_PX, SLICE_COUNT, 1])\n",
    "\n",
    "    conv1 = tf.nn.relu(conv3d(x, weights['W_conv1']) + biases['b_conv1'])\n",
    "    conv1 = maxpool3d(conv1)\n",
    "\n",
    "\n",
    "    conv2 = tf.nn.relu(conv3d(conv1, weights['W_conv2']) + biases['b_conv2'])\n",
    "    conv2 = maxpool3d(conv2)\n",
    "    #conv3 = tf.nn.relu(conv3d(conv2, weights['W_conv3']) + biases['b_conv3'])\n",
    "    #conv3 = maxpool3d(conv3)\n",
    "    #conv4 = tf.nn.relu(conv3d(conv3, weights['W_conv4']) + biases['b_conv4'])\n",
    "    #conv4 = maxpool3d(conv4)\n",
    "\n",
    "    #fc = tf.reshape(conv4,[-1, 163840])\n",
    "    fc = tf.reshape(conv2,[-1, 54080])\n",
    "    fc = tf.nn.relu(tf.matmul(fc, weights['W_fc'])+biases['b_fc'])\n",
    "    fc = tf.nn.dropout(fc, keep_rate)\n",
    "\n",
    "    output = tf.matmul(fc, weights['out'])+biases['out']\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-6894f1445e9c>:10: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "much_data = np.load('muchdata-50-50-20.npy')\n",
    "train_data = much_data[:-100]\n",
    "validation_data = much_data[-297:]\n",
    "\n",
    "\n",
    "def train_neural_network(x):\n",
    "    prediction = convolutional_neural_network(x)\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-7).minimize(cost)\n",
    "    init=tf.initialize_all_variables()\n",
    "    #saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        successful_runs = 0\n",
    "        total_runs = 0\n",
    "        hm_epochs = 20\n",
    "        \n",
    "        for epoch in range(hm_epochs):\n",
    "            for data in train_data:\n",
    "                total_runs += 1\n",
    "                try:\n",
    "                    epoch_loss = 0\n",
    "                    X = data[0]\n",
    "                    Y = data[1]\n",
    "                    #print(Y)\n",
    "                    _, c = sess.run([optimizer, cost], feed_dict={x: X, y: Y})\n",
    "                    epoch_loss += c\n",
    "                    successful_run += 1\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "                    #print(str(e))\n",
    "            \n",
    "            print('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "\n",
    "            correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "\n",
    "            print('Accuracy:',accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]}))\n",
    "            \n",
    "        print('Done. Finishing accuracy:')\n",
    "        print('Accuracy:',accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]}))\n",
    "        \n",
    "        print('fitment percent:',successful_runs/total_runs)\n",
    "        #saver.save(sess,'D:/3dconvnetmodel.ckpt')\n",
    "\n",
    "\n",
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No variables to save",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-67ab2b43e838>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[1;31m#Restore variables from disk.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"D:/3dconvnetmodel.ckpt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number)\u001b[0m\n\u001b[1;32m   1054\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pad_step_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_step_number\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1075\u001b[0m           \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1077\u001b[0;31m           \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No variables to save\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1078\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_empty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m       self.saver_def = self._builder.build(\n",
      "\u001b[0;31mValueError\u001b[0m: No variables to save"
     ]
    }
   ],
   "source": [
    "#with open('D:/mysubmission_file.csv','w') as f:\n",
    "    #f.write('id,label\\n')\n",
    "\n",
    "    #Restore variables from disk.\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,\"D:/3dconvnetmodel.ckpt\")\n",
    "    print(\"Model restored.\")            \n",
    "#with open('submission_file.csv','a') as f:\n",
    "    #for data in test_data:\n",
    "        #img_num = data[1]\n",
    "        #img_data = data[0]\n",
    "        #orig = img_data\n",
    "        #data = img_data.reshape(IMG_SIZE,IMG_SIZE,1)\n",
    "        #model_out = model.predict([data])[0]\n",
    "        #f.write('{},{}\\n'.format(img_num,model_out[1]))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Saver' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-b97902d8a7a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[1;31m#orig = img_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIMG_SIZE_PX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mIMG_SIZE_PX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmodel_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{},{}\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_out\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Saver' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "test_data=np.load('D:/testdata-50-50-20.npy')\n",
    "with open('submission_file.csv','a') as f:\n",
    "    for data in test_data:\n",
    "        x = data[0]\n",
    "        y = data[1]\n",
    "        #orig = img_data\n",
    "        data = x.reshape(IMG_SIZE_PX,IMG_SIZE_PX,20)\n",
    "        model_out = saver.predict([data])[0]\n",
    "        f.write('{},{}\\n'.format(y,model_out[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "No input data! Please add an 'input_data' layer to your model (or add your input data placeholder to tf.GraphKeys.INPUTS collection).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-60282ed705ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0mconvolutional_neural_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-60282ed705ac>\u001b[0m in \u001b[0;36mconvolutional_neural_network\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'targets'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensorboard_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'log'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[1;31m#return output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mmuch_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:/muchdata-50-50-20.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tflearn\\models\\dnn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, network, clip_gradients, tensorboard_verbose, tensorboard_dir, checkpoint_path, best_checkpoint_path, max_checkpoints, session, best_val_accuracy)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mINPUTS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             raise Exception(\"No input data! Please add an 'input_data' layer \"\n\u001b[0m\u001b[1;32m     70\u001b[0m                             \u001b[1;34m\"to your model (or add your input data \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                             \"placeholder to tf.GraphKeys.INPUTS collection).\")\n",
      "\u001b[0;31mException\u001b[0m: No input data! Please add an 'input_data' layer to your model (or add your input data placeholder to tf.GraphKeys.INPUTS collection)."
     ]
    }
   ],
   "source": [
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tflearn.layers.estimator import regression\n",
    "\n",
    "IMG_SIZE_PX=50\n",
    "SLICE_COUNT=20\n",
    "\n",
    "n_classes=2\n",
    "\n",
    "x=tf.placeholder('float')\n",
    "y=tf.placeholder('float')\n",
    "\n",
    "keep_rate=0.8\n",
    "keep_prob=tf.placeholder(tf.float32)\n",
    "\n",
    "def conv3d(x, W):\n",
    "    return tf.nn.conv3d(x, W, strides=[1,1,1,1,1], padding='SAME')\n",
    "\n",
    "def maxpool3d(x):\n",
    "    \n",
    "    return tf.nn.maxpool3d(x, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='SAME')\n",
    "\n",
    "def convolutional_neural_network(x):\n",
    "    \n",
    "    weights = {'W_conv1':tf.Variable(tf.random_normal([3,3,3,1,32])),\n",
    "               \n",
    "               'W_conv2':tf.Variable(tf.random_normal([3,3,3,32,64])),\n",
    "               #'W_conv3':tf.Variable(tf.random_normal([3,3,3,64,128])),\n",
    "               #'W_conv4':tf.Variable(tf.random_normal([5,5,5,128,256])),\n",
    "               #'W_conv5':tf.Variable(tf.random_normal([5,5,5,256,512])),\n",
    "               #'W_conv6':tf.Variable(tf.random_normal([5,5,5,512,1024])),\n",
    "              # 'W_conv7':tf.Variable(tf.random_normal([5,5,5,1024,2048])),\n",
    "               \n",
    "               'W_fc':tf.Variable(tf.random_normal([ 54080 ,1024])),\n",
    "               'out':tf.Variable(tf.random_normal([1024, n_classes]))}\n",
    "\n",
    "    biases = {'b_conv1':tf.Variable(tf.random_normal([32])),\n",
    "               'b_conv2':tf.Variable(tf.random_normal([64])),\n",
    "               #'b_conv3':tf.Variable(tf.random_normal([128])),\n",
    "               #'b_conv4':tf.Variable(tf.random_normal([256])),\n",
    "               #'b_conv5':tf.Variable(tf.random_normal([512])),\n",
    "               #'b_conv6':tf.Variable(tf.random_normal([1024])),\n",
    "               #'b_conv7':tf.Variable(tf.random_normal([2048])),\n",
    "               'b_fc':tf.Variable(tf.random_normal([1024])),\n",
    "               'out':tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "    \n",
    "    x = tf.reshape(x, shape=[-1, IMG_SIZE_PX, IMG_SIZE_PX, SLICE_COUNT, 1])\n",
    "\n",
    "    conv1 = tf.nn.relu(conv3d(x, weights['W_conv1']) + biases['b_conv1'])\n",
    "    conv1 = maxpool3d(conv1)\n",
    "    conv2 = tf.nn.relu(conv3d(conv1, weights['W_conv2']) + biases['b_conv2'])\n",
    "    conv2 = maxpool3d(conv2)\n",
    "    #conv3 = tf.nn.relu(conv3d(conv2, weights['W_conv3']) + biases['b_conv3'])\n",
    "    #conv3 = maxpool3d(conv3)\n",
    "    #conv4 = tf.nn.relu(conv3d(conv3, weights['W_conv4']) + biases['b_conv4'])\n",
    "    #conv4 = maxpool3d(conv4)\n",
    "    #conv5 = tf.nn.relu(conv3d(conv4, weights['W_conv5']) + biases['b_conv5'])\n",
    "    #conv5 = maxpool3d(conv5)\n",
    "    #conv6 = tf.nn.relu(conv3d(conv5, weights['W_conv6']) + biases['b_conv6'])\n",
    "    #conv6 = maxpool3d(conv6)\n",
    "    #conv7 = tf.nn.relu(conv3d(conv6, weights['W_conv7']) + biases['b_conv7'])\n",
    "    #conv7 = maxpool3d(conv7)\n",
    "    \n",
    "    \n",
    "\n",
    "    fc = tf.reshape(conv2,[-1,  54080 ])\n",
    "    fc = tf.nn.relu(tf.matmul(fc, weights['W_fc'])+biases['b_fc'])\n",
    "    fc = tf.nn.dropout(fc, keep_rate)\n",
    "\n",
    "    output = tf.matmul(fc, weights['out'])+biases['out']\n",
    "    output = regression(output, optimizer='adam', learning_rate=1e-7, loss='categorical_crossentropy', name='targets')\n",
    "    \n",
    "    model = tflearn.DNN(output, tensorboard_dir='log')\n",
    "    #return output\n",
    "    much_data = np.load('D:/muchdata-50-50-20.npy')\n",
    "    train_data = much_data[-100:]\n",
    "    test_data = much_data[-20:]\n",
    "    if os.path.exists('{}.meta'.format(MODEL_NAME)):\n",
    "        model.load(MODEL_NAME)\n",
    "        print('model loaded!')\n",
    "    X = np.array([i[0] for i in train_data]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "    Y = [i[1] for i in train_data]\n",
    "\n",
    "    test_x = np.array([i[0] for i in test_data]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "    test_y = [i[1] for i in test_data]\n",
    "\n",
    "    model.fit({'input': X}, {'targets': Y}, n_epoch=10, validation_set=({'input': test_x}, {'targets': test_y}), \n",
    "    snapshot_step=500, show_metric=True, run_id=MODEL_NAME)\n",
    "\n",
    "    model.save(MODEL_NAME)\n",
    "convolutional_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Incoming Tensor shape must be 5-D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-206bc0f39b63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mconvnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIMG_SIZE_PX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIMG_SIZE_PX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSLICE_COUNT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mconvnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mconvnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_pool_3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tflearn\\layers\\conv.py\u001b[0m in \u001b[0;36mconv_3d\u001b[0;34m(incoming, nb_filter, filter_size, strides, padding, activation, bias, weights_init, bias_init, regularizer, weight_decay, trainable, restore, reuse, scope, name)\u001b[0m\n\u001b[1;32m    739\u001b[0m     \"\"\"\n\u001b[1;32m    740\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_incoming_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mincoming\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m     \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Incoming Tensor shape must be 5-D\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m     filter_size = utils.autoformat_filter_conv3d(filter_size,\n\u001b[1;32m    743\u001b[0m                                                  \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Incoming Tensor shape must be 5-D"
     ]
    }
   ],
   "source": [
    "import tflearn\n",
    "from tflearn import conv_3d,max_pool_3d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "\n",
    "IMG_SIZE_PX=50\n",
    "SLICE_COUNT=20\n",
    "\n",
    "convnet = input_data(shape=[3, IMG_SIZE_PX, IMG_SIZE_PX,SLICE_COUNT,1])\n",
    "\n",
    "convnet = conv_3d(convnet, 32, 3, activation='relu')\n",
    "convnet = max_pool_3d(convnet, 3)\n",
    "\n",
    "convnet = conv_3d(convnet, 64, 3, activation='relu')\n",
    "convnet = max_pool_3d(convnet, 3)\n",
    "\n",
    "convnet = fully_connected(convnet, 1024, activation='relu')\n",
    "convnet = dropout(convnet, 0.8)\n",
    "\n",
    "convnet = fully_connected(convnet, 2, activation='softmax')\n",
    "convnet = regression(convnet, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n",
    "\n",
    "model = tflearn.DNN(convnet, tensorboard_dir='log')\n",
    "\n",
    "if os.path.exists('{}.meta'.format(MODEL_NAME)):\n",
    "    model.load(MODEL_NAME)\n",
    "    print('model loaded!')\n",
    "\n",
    "train = train_data[-100:]\n",
    "test = train_data[-20:]\n",
    "\n",
    "X = np.array([i[0] for i in train]).reshape(-1,IMG_SIZE_PX,IMG_SIZE_PX,SLICE_COUNT,1)\n",
    "Y = [i[1] for i in train]\n",
    "\n",
    "test_x = np.array([i[0] for i in test]).reshape(-1,IMG_SIZE_PX,IMG_SIZE_PX,SLICE_COUNT,1)\n",
    "test_y = [i[1] for i in test]\n",
    "\n",
    "model.fit({'input': X}, {'targets': Y}, n_epoch=3, validation_set=({'input': test_x}, {'targets': test_y}), \n",
    "    snapshot_step=500, show_metric=True, run_id=MODEL_NAME)\n",
    "model.save(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-16-a4370a69029d>:82: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Epoch 1 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.65\n",
      "Epoch 2 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.65\n",
      "Epoch 3 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.6\n",
      "Epoch 4 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.65\n",
      "Epoch 5 completed out of 20 loss: 17885824.0\n",
      "Accuracy: 0.7\n",
      "Epoch 6 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.7\n",
      "Epoch 7 completed out of 20 loss: 250131568.0\n",
      "Accuracy: 0.8\n",
      "Epoch 8 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.85\n",
      "Epoch 9 completed out of 20 loss: 215342176.0\n",
      "Accuracy: 0.95\n",
      "Epoch 10 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.95\n",
      "Epoch 11 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.95\n",
      "Epoch 12 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.85\n",
      "Epoch 13 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.95\n",
      "Epoch 14 completed out of 20 loss: 0.0\n",
      "Accuracy: 1.0\n",
      "Epoch 15 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.95\n",
      "Epoch 16 completed out of 20 loss: 218914496.0\n",
      "Accuracy: 0.95\n",
      "Epoch 17 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.9\n",
      "Epoch 18 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.95\n",
      "Epoch 19 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.95\n",
      "Epoch 20 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.8\n",
      "Done. Finishing accuracy:\n",
      "Accuracy: 0.8\n",
      "fitment percent: 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "IMG_SIZE_PX=50\n",
    "SLICE_COUNT=20\n",
    "\n",
    "n_classes=2\n",
    "\n",
    "x=tf.placeholder('float')\n",
    "y=tf.placeholder('float')\n",
    "\n",
    "keep_rate=0.8\n",
    "keep_prob=tf.placeholder(tf.float32)\n",
    "\n",
    "def conv3d(x, W):\n",
    "    return tf.nn.conv3d(x, W, strides=[1,1,1,1,1], padding='SAME')\n",
    "\n",
    "def maxpool3d(x):\n",
    "    \n",
    "    return tf.nn.max_pool3d(x, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='SAME')\n",
    "\n",
    "def convolutional_neural_network(x):\n",
    "    \n",
    "    weights = {'W_conv1':tf.Variable(tf.random_normal([3,3,3,1,32])),\n",
    "               \n",
    "               'W_conv2':tf.Variable(tf.random_normal([3,3,3,32,64])),\n",
    "               #'W_conv3':tf.Variable(tf.random_normal([3,3,3,64,128])),\n",
    "               #'W_conv4':tf.Variable(tf.random_normal([5,5,5,128,256])),\n",
    "               #'W_conv5':tf.Variable(tf.random_normal([5,5,5,256,512])),\n",
    "               #'W_conv6':tf.Variable(tf.random_normal([5,5,5,512,1024])),\n",
    "              # 'W_conv7':tf.Variable(tf.random_normal([5,5,5,1024,2048])),\n",
    "               \n",
    "               'W_fc':tf.Variable(tf.random_normal([ 54080 ,1024])),\n",
    "               'out':tf.Variable(tf.random_normal([1024, n_classes]))}\n",
    "\n",
    "    biases = {'b_conv1':tf.Variable(tf.random_normal([32])),\n",
    "               'b_conv2':tf.Variable(tf.random_normal([64])),\n",
    "               #'b_conv3':tf.Variable(tf.random_normal([128])),\n",
    "               #'b_conv4':tf.Variable(tf.random_normal([256])),\n",
    "               #'b_conv5':tf.Variable(tf.random_normal([512])),\n",
    "               #'b_conv6':tf.Variable(tf.random_normal([1024])),\n",
    "               #'b_conv7':tf.Variable(tf.random_normal([2048])),\n",
    "               'b_fc':tf.Variable(tf.random_normal([1024])),\n",
    "               'out':tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "    \n",
    "    x = tf.reshape(x, shape=[-1, IMG_SIZE_PX, IMG_SIZE_PX, SLICE_COUNT, 1])\n",
    "\n",
    "    conv1 = tf.nn.relu(conv3d(x, weights['W_conv1']) + biases['b_conv1'])\n",
    "    conv1 = maxpool3d(conv1)\n",
    "    conv2 = tf.nn.relu(conv3d(conv1, weights['W_conv2']) + biases['b_conv2'])\n",
    "    conv2 = maxpool3d(conv2)\n",
    "    #conv3 = tf.nn.relu(conv3d(conv2, weights['W_conv3']) + biases['b_conv3'])\n",
    "    #conv3 = maxpool3d(conv3)\n",
    "    #conv4 = tf.nn.relu(conv3d(conv3, weights['W_conv4']) + biases['b_conv4'])\n",
    "    #conv4 = maxpool3d(conv4)\n",
    "    #conv5 = tf.nn.relu(conv3d(conv4, weights['W_conv5']) + biases['b_conv5'])\n",
    "    #conv5 = maxpool3d(conv5)\n",
    "    #conv6 = tf.nn.relu(conv3d(conv5, weights['W_conv6']) + biases['b_conv6'])\n",
    "    #conv6 = maxpool3d(conv6)\n",
    "    #conv7 = tf.nn.relu(conv3d(conv6, weights['W_conv7']) + biases['b_conv7'])\n",
    "    #conv7 = maxpool3d(conv7)\n",
    "    \n",
    "    \n",
    "\n",
    "    fc = tf.reshape(conv2,[-1,  54080 ])\n",
    "    fc = tf.nn.relu(tf.matmul(fc, weights['W_fc'])+biases['b_fc'])\n",
    "    fc = tf.nn.dropout(fc, keep_rate)\n",
    "\n",
    "    output = tf.matmul(fc, weights['out'])+biases['out']\n",
    "    \n",
    "\n",
    "    return output\n",
    "much_data = np.load('D:/muchdata-50-50-20.npy')\n",
    "train_data = much_data[-100:]\n",
    "validation_data = much_data[-20:]\n",
    "def train_neural_network(x):\n",
    "    prediction = convolutional_neural_network(x)\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        successful_runs = 0\n",
    "        total_runs = 0\n",
    "        hm_epochs = 20\n",
    "        \n",
    "        for epoch in range(hm_epochs):\n",
    "            for data in train_data:\n",
    "                total_runs += 1\n",
    "                epoch_loss = 0\n",
    "                X = data[0]\n",
    "                Y = data[1]\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: X, y: Y})\n",
    "                epoch_loss += c\n",
    "                successful_runs += 1\n",
    "            print('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "            correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "            print('Accuracy:',accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]}))\n",
    "        print('Done. Finishing accuracy:')\n",
    "        print('Accuracy:',accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]}))\n",
    "        \n",
    "        print('fitment percent:',successful_runs/total_runs)    \n",
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-9f85ebfafd8a>:82: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Epoch 1 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.65\n",
      "Epoch 2 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.7\n",
      "Epoch 3 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.65\n",
      "Epoch 4 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.6\n",
      "Epoch 5 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.65\n",
      "Epoch 6 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.55\n",
      "Epoch 7 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.6\n",
      "Epoch 8 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.55\n",
      "Epoch 9 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.6\n",
      "Epoch 10 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.6\n",
      "Epoch 11 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.65\n",
      "Epoch 12 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.65\n",
      "Epoch 13 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.7\n",
      "Epoch 14 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.65\n",
      "Epoch 15 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.65\n",
      "Epoch 16 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.65\n",
      "Epoch 17 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.6\n",
      "Epoch 18 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.6\n",
      "Epoch 19 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.7\n",
      "Epoch 20 completed out of 20 loss: 0.0\n",
      "Accuracy: 0.6\n",
      "Done. Finishing accuracy:\n",
      "Accuracy: 0.6\n",
      "fitment percent: 0.0\n",
      "WARNING:tensorflow:Error encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'NoneType' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'NoneType' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "IMG_SIZE_PX=50\n",
    "SLICE_COUNT=20\n",
    "\n",
    "n_classes=2\n",
    "\n",
    "x=tf.placeholder('float')\n",
    "y=tf.placeholder('float')\n",
    "\n",
    "keep_rate=0.8\n",
    "keep_prob=tf.placeholder(tf.float32)\n",
    "\n",
    "def conv3d(x, W):\n",
    "    return tf.nn.conv3d(x, W, strides=[1,1,1,1,1], padding='SAME')\n",
    "\n",
    "def maxpool3d(x):\n",
    "    \n",
    "    return tf.nn.max_pool3d(x, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='SAME')\n",
    "\n",
    "def convolutional_neural_network(x):\n",
    "    \n",
    "    weights = {'W_conv1':tf.Variable(tf.random_normal([3,3,3,1,32])),\n",
    "               \n",
    "               'W_conv2':tf.Variable(tf.random_normal([3,3,3,32,64])),\n",
    "               #'W_conv3':tf.Variable(tf.random_normal([3,3,3,64,128])),\n",
    "               #'W_conv4':tf.Variable(tf.random_normal([5,5,5,128,256])),\n",
    "               #'W_conv5':tf.Variable(tf.random_normal([5,5,5,256,512])),\n",
    "               #'W_conv6':tf.Variable(tf.random_normal([5,5,5,512,1024])),\n",
    "              # 'W_conv7':tf.Variable(tf.random_normal([5,5,5,1024,2048])),\n",
    "               \n",
    "               'W_fc':tf.Variable(tf.random_normal([ 54080 ,1024])),\n",
    "               'out':tf.Variable(tf.random_normal([1024, n_classes]))}\n",
    "\n",
    "    biases = {'b_conv1':tf.Variable(tf.random_normal([32])),\n",
    "               'b_conv2':tf.Variable(tf.random_normal([64])),\n",
    "               #'b_conv3':tf.Variable(tf.random_normal([128])),\n",
    "               #'b_conv4':tf.Variable(tf.random_normal([256])),\n",
    "               #'b_conv5':tf.Variable(tf.random_normal([512])),\n",
    "               #'b_conv6':tf.Variable(tf.random_normal([1024])),\n",
    "               #'b_conv7':tf.Variable(tf.random_normal([2048])),\n",
    "               'b_fc':tf.Variable(tf.random_normal([1024])),\n",
    "               'out':tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "    \n",
    "    x = tf.reshape(x, shape=[-1, IMG_SIZE_PX, IMG_SIZE_PX, SLICE_COUNT, 1])\n",
    "\n",
    "    conv1 = tf.nn.relu(conv3d(x, weights['W_conv1']) + biases['b_conv1'])\n",
    "    conv1 = maxpool3d(conv1)\n",
    "    conv2 = tf.nn.relu(conv3d(conv1, weights['W_conv2']) + biases['b_conv2'])\n",
    "    conv2 = maxpool3d(conv2)\n",
    "    #conv3 = tf.nn.relu(conv3d(conv2, weights['W_conv3']) + biases['b_conv3'])\n",
    "    #conv3 = maxpool3d(conv3)\n",
    "    #conv4 = tf.nn.relu(conv3d(conv3, weights['W_conv4']) + biases['b_conv4'])\n",
    "    #conv4 = maxpool3d(conv4)\n",
    "    #conv5 = tf.nn.relu(conv3d(conv4, weights['W_conv5']) + biases['b_conv5'])\n",
    "    #conv5 = maxpool3d(conv5)\n",
    "    #conv6 = tf.nn.relu(conv3d(conv5, weights['W_conv6']) + biases['b_conv6'])\n",
    "    #conv6 = maxpool3d(conv6)\n",
    "    #conv7 = tf.nn.relu(conv3d(conv6, weights['W_conv7']) + biases['b_conv7'])\n",
    "    #conv7 = maxpool3d(conv7)\n",
    "    \n",
    "    \n",
    "\n",
    "    fc = tf.reshape(conv2,[-1,  54080 ])\n",
    "    fc = tf.nn.relu(tf.matmul(fc, weights['W_fc'])+biases['b_fc'])\n",
    "    fc = tf.nn.dropout(fc, keep_rate)\n",
    "\n",
    "    output = tf.matmul(fc, weights['out'])+biases['out']\n",
    "    \n",
    "\n",
    "    return output\n",
    "saver = tf.train.Saver()\n",
    "much_data = np.load('D:/muchdata-50-50-20.npy')\n",
    "test_data=np.load('D:/testdata-50-50-20.npy')\n",
    "train_data = much_data[-990:]\n",
    "validation_data = test_data[-198:]\n",
    "def train_neural_network(x):\n",
    "    prediction = convolutional_neural_network(x)\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-7).minimize(cost)\n",
    "    init=tf.initialize_all_variables()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        successful_runs = 0\n",
    "        total_runs = 0\n",
    "        hm_epochs = 20\n",
    "        \n",
    "        for epoch in range(hm_epochs):\n",
    "            for data in train_data:\n",
    "                total_runs += 1\n",
    "                try:\n",
    "                    epoch_loss = 0\n",
    "                    X = data[0]\n",
    "                    Y = data[1]\n",
    "                    #print(Y)\n",
    "                    _, c = sess.run([optimizer, cost], feed_dict={x: X, y: Y})\n",
    "                    epoch_loss += c\n",
    "                    successful_run += 1\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "                    #print(str(e))\n",
    "            \n",
    "            print('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "\n",
    "            correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "\n",
    "            print('Accuracy:',accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]}))\n",
    "            \n",
    "        print('Done. Finishing accuracy:')\n",
    "        print('Accuracy:',accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]}))\n",
    "        \n",
    "        print('fitment percent:',successful_runs/total_runs)\n",
    "        saver.save(sess,'D:/3dconvnetmodel.ckpt')\n",
    "\n",
    "\n",
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "-100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2133\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2134\u001b[0;31m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2135\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4433)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4279)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\src\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:13742)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\src\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:13696)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: -100",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-195674ff33dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlabels_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:/stage1_labels.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2057\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2059\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2060\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2064\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2065\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2066\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2067\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2068\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1384\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1386\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1387\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3542\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3543\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3544\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3545\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\pandas\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2134\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2136\u001b[0;31m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4433)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4279)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\src\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:13742)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\src\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:13696)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: -100"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "labels_df=pd.read_csv('D:/stage1_labels.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-38-7bfed065f431>:4: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-7bfed065f431>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m198\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mimg_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mIMG_SIZE_PX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mIMG_SIZE_PX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSLICE_COUNT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mclassification\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "test_data=np.load('D:/testdata-50-50-20.npy')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    for data in enumerate(test_data[:198]):\n",
    "        img_data = data[0]\n",
    "        data = img_data.reshape(-1,IMG_SIZE_PX,IMG_SIZE_PX,SLICE_COUNT,1)\n",
    "        feed_dict = {x: [data[0]]}\n",
    "        classification = tf.run(tf.argmax(y, 1), feed_dict)\n",
    "        print(classification)\n",
    "        #data = x.reshape(-1,IMG_SIZE_PX,IMG_SIZE_PX,20,1)\n",
    "        #print(sess.run(tf.argmax(y, 1), feed_dict={x: data[0]}))\n",
    "        #prediction=tf.argmax(y,1)\n",
    "        #print(prediction.eval(feed_dict={x:[i[0] for i in test_data] }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-417a8c7de596>:85: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Epoch 1 completed out of 3 loss: 47426440.0\n",
      "Accuracy: 0.691919\n",
      "Epoch 2 completed out of 3 loss: 0.0\n",
      "Accuracy: 0.732323\n",
      "Epoch 3 completed out of 3 loss: 0.0\n",
      "Accuracy: 0.80303\n",
      "Done. Finishing accuracy:\n",
      "Accuracy: 0.772727\n",
      "fitment percent: 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "IMG_SIZE_PX=50\n",
    "SLICE_COUNT=20\n",
    "\n",
    "n_classes=2\n",
    "\n",
    "x=tf.placeholder('float')\n",
    "y=tf.placeholder('float')\n",
    "\n",
    "keep_rate=0.8\n",
    "keep_prob=tf.placeholder(tf.float32)\n",
    "\n",
    "def conv3d(x, W):\n",
    "    return tf.nn.conv3d(x, W, strides=[1,1,1,1,1], padding='SAME')\n",
    "\n",
    "def maxpool3d(x):\n",
    "    \n",
    "    return tf.nn.max_pool3d(x, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='SAME')\n",
    "\n",
    "def convolutional_neural_network(x):\n",
    "    \n",
    "    weights = {'W_conv1':tf.Variable(tf.random_normal([3,3,3,1,32])),\n",
    "               \n",
    "               'W_conv2':tf.Variable(tf.random_normal([3,3,3,32,64])),\n",
    "               #'W_conv3':tf.Variable(tf.random_normal([3,3,3,64,128])),\n",
    "               #'W_conv4':tf.Variable(tf.random_normal([5,5,5,128,256])),\n",
    "               #'W_conv5':tf.Variable(tf.random_normal([5,5,5,256,512])),\n",
    "               #'W_conv6':tf.Variable(tf.random_normal([5,5,5,512,1024])),\n",
    "              # 'W_conv7':tf.Variable(tf.random_normal([5,5,5,1024,2048])),\n",
    "               \n",
    "               'W_fc':tf.Variable(tf.random_normal([ 54080 ,1024])),\n",
    "               'out':tf.Variable(tf.random_normal([1024, n_classes]))}\n",
    "\n",
    "    biases = {'b_conv1':tf.Variable(tf.random_normal([32])),\n",
    "               'b_conv2':tf.Variable(tf.random_normal([64])),\n",
    "               #'b_conv3':tf.Variable(tf.random_normal([128])),\n",
    "               #'b_conv4':tf.Variable(tf.random_normal([256])),\n",
    "               #'b_conv5':tf.Variable(tf.random_normal([512])),\n",
    "               #'b_conv6':tf.Variable(tf.random_normal([1024])),\n",
    "               #'b_conv7':tf.Variable(tf.random_normal([2048])),\n",
    "               'b_fc':tf.Variable(tf.random_normal([1024])),\n",
    "               'out':tf.Variable(tf.random_normal([n_classes]))}\n",
    "\n",
    "    \n",
    "    x = tf.reshape(x, shape=[-1, IMG_SIZE_PX, IMG_SIZE_PX, SLICE_COUNT, 1])\n",
    "\n",
    "    conv1 = tf.nn.relu(conv3d(x, weights['W_conv1']) + biases['b_conv1'])\n",
    "    conv1 = maxpool3d(conv1)\n",
    "    conv2 = tf.nn.relu(conv3d(conv1, weights['W_conv2']) + biases['b_conv2'])\n",
    "    conv2 = maxpool3d(conv2)\n",
    "    #conv3 = tf.nn.relu(conv3d(conv2, weights['W_conv3']) + biases['b_conv3'])\n",
    "    #conv3 = maxpool3d(conv3)\n",
    "    #conv4 = tf.nn.relu(conv3d(conv3, weights['W_conv4']) + biases['b_conv4'])\n",
    "    #conv4 = maxpool3d(conv4)\n",
    "    #conv5 = tf.nn.relu(conv3d(conv4, weights['W_conv5']) + biases['b_conv5'])\n",
    "    #conv5 = maxpool3d(conv5)\n",
    "    #conv6 = tf.nn.relu(conv3d(conv5, weights['W_conv6']) + biases['b_conv6'])\n",
    "    #conv6 = maxpool3d(conv6)\n",
    "    #conv7 = tf.nn.relu(conv3d(conv6, weights['W_conv7']) + biases['b_conv7'])\n",
    "    #conv7 = maxpool3d(conv7)\n",
    "    \n",
    "    \n",
    "\n",
    "    fc = tf.reshape(conv2,[-1,  54080 ])\n",
    "    fc = tf.nn.relu(tf.matmul(fc, weights['W_fc'])+biases['b_fc'])\n",
    "    fc = tf.nn.dropout(fc, keep_rate)\n",
    "\n",
    "    output = tf.matmul(fc, weights['out'])+biases['out']\n",
    "    \n",
    "\n",
    "    return output\n",
    "much_data = np.load('D:/muchdata-50-50-20.npy')\n",
    "test_data=np.load('D:/testdata-50-50-20.npy')\n",
    "train_data = much_data[-990:]\n",
    "validation_data = much_data[-198:]\n",
    "def train_neural_network(x):\n",
    "    with tf.Session() as sess:\n",
    "        prediction = convolutional_neural_network(x)\n",
    "        cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y) )\n",
    "        optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "        saver = tf.train.Saver()\n",
    "    \n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        successful_runs = 0\n",
    "        total_runs = 0\n",
    "        hm_epochs = 3\n",
    "        \n",
    "        for epoch in range(hm_epochs):\n",
    "            for data in train_data:\n",
    "                total_runs += 1\n",
    "                epoch_loss = 0\n",
    "                X = data[0]\n",
    "                Y = data[1]\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: X, y: Y})\n",
    "                epoch_loss += c\n",
    "                successful_runs += 1\n",
    "            print('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "            correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "            print('Accuracy:',accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]}))\n",
    "        print('Done. Finishing accuracy:')\n",
    "        print('Accuracy:',accuracy.eval({x:[i[0] for i in validation_data], y:[i[1] for i in validation_data]}))\n",
    "        \n",
    "        print('fitment percent:',successful_runs/total_runs) \n",
    "        saver.save(sess,'D:/3dconvnetmodel.ckpt')\n",
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "__exit__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-22a36f1d69d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0msaver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'D:/3dconvmodel.ckpt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: __exit__"
     ]
    }
   ],
   "source": [
    "with tf.Session as sess:\n",
    "    saver=tf.train.Saver()\n",
    "    saver.restore(sess,'D:/3dconvmodel.ckpt')\n",
    "    sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
